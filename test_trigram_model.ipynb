{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from nltk.util import ngrams\n",
    "def get_ngrams(sequence, n):\n",
    "    \"\"\"\n",
    "    COMPLETE THIS FUNCTION (PART 1)\n",
    "    Given a sequence, this function should return a list of n-grams, where each n-gram is a Python tuple.\n",
    "    This should work for arbitrary values of 1 <= n < len(sequence).\n",
    "    \"\"\"\n",
    "    return list(ngrams(tokenize,n))\n",
    "    \n",
    "    sequence = deque(sequence)\n",
    "    if n>1:\n",
    "        sequence.extendleft(['START']*(n-1))\n",
    "    else:\n",
    "        sequence.extendleft(['START'])\n",
    "    sequence.extend(['STOP'])\n",
    "    sequence = list(sequence)\n",
    "    ngrams = []\n",
    "    for seq in range(0,len(sequence)-(n-1)):\n",
    "        ngrams.append(tuple(sequence[seq:seq+n]))\n",
    "#     print(ngrams)\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def count_ngrams(corpus):\n",
    "    \"\"\"\n",
    "    COMPLETE THIS METHOD (PART 2)\n",
    "    Given a corpus iterator, populate dictionaries of unigram, bigram,\n",
    "    and trigram counts. \n",
    "    \"\"\"\n",
    "    unigramcounts = Counter(get_ngrams(corpus,1))\n",
    "    bigramcounts = Counter(get_ngrams(corpus,2))\n",
    "    trigramcounts = Counter(get_ngrams(corpus,3))\n",
    "    total_words = sum(unigramcounts.values())\n",
    "    return unigramcounts, bigramcounts, trigramcounts, total_words\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_trigram_probability(self,trigram):\n",
    "    \"\"\"\n",
    "    COMPLETE THIS METHOD (PART 3)\n",
    "    Returns the raw (unsmoothed) trigram probability\n",
    "    \"\"\"\n",
    "    return round(trigramcounts[trigram]/bigramcounts[trigram[0:2]],2)\n",
    "\n",
    "def raw_bigram_probability(self, bigram):\n",
    "    \"\"\"\n",
    "    COMPLETE THIS METHOD (PART 3)\n",
    "    Returns the raw (unsmoothed) bigram probability\n",
    "    \"\"\"\n",
    "    return round(bigramcounts[bigram]/unigramcounts[bigram[0]],2) #no backing off to unigram probability\n",
    "\n",
    "def raw_unigram_probability(self, unigram):\n",
    "    \"\"\"\n",
    "    COMPLETE THIS METHOD (PART 3)\n",
    "    Returns the raw (unsmoothed) unigram probability.\n",
    "    \"\"\"\n",
    "    #hint: recomputing the denominator every time the method is called\n",
    "    # can be slow! You might want to compute the total number of words once, \n",
    "    # store in the TrigramModel instance, and then re-use it.  \n",
    "    return round(unigramcounts[unigram]/total_words,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothed_trigram_probability(self, trigram):\n",
    "    \"\"\"\n",
    "    COMPLETE THIS METHOD (PART 4)\n",
    "    Returns the smoothed trigram probability (using linear interpolation). \n",
    "    \"\"\"\n",
    "    lambda1 = 1/3.0\n",
    "    lambda2 = 1/3.0\n",
    "    lambda3 = 1/3.0\n",
    "    return lambda1 * raw_trigram_probability(trigram) + lambda2 * raw_bigram_probability(trigram[1:]) + lambda3 * raw_unigram_probability(trigram[2])\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def sentence_logprob(sentence):\n",
    "    trigram = get_ngrams(sentence,3)\n",
    "    sent_prob = 0\n",
    "    for tri in trigram:\n",
    "        sent_prob+= math.log2(smoothed_trigram_probability(tri))\n",
    "    return sent_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(corpus):\n",
    "    sent_logprob = 0\n",
    "    for sentence in corpus:\n",
    "        sent_logprob+=math.log2(sentence_logprob(sentence))\n",
    "    norm_sent_logprob = sent_logprob/total_words\n",
    "    return 2**(-norm_sent_logprob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'ngrams' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-08faef96c7b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"natural\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"language\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"processing\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"language\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"processed\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"natural\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"natural\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"language\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"processing\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_ngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"========================\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_ngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-5aab369b23bb>\u001b[0m in \u001b[0;36mget_ngrams\u001b[1;34m(sequence, n)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mwork\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marbitrary\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mof\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \"\"\"\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0msequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeque\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'ngrams' referenced before assignment"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    s = [\"natural\",\"language\",\"processing\",\"language\",\"processed\",\"natural\",\"natural\",\"language\",\"processing\"]\n",
    "    u = get_ngrams(s,1)\n",
    "    print(\"========================\")\n",
    "    b = get_ngrams(s,2)\n",
    "    print(\"========================\")\n",
    "    t = get_ngrams(s,3)\n",
    "    print(\"\\n\\n\")\n",
    "    unigramcounts, bigramcounts, trigramcounts, total_words = count_ngrams(s)\n",
    "    print(t,\"\\n\")\n",
    "    print(trigramcounts)\n",
    "    print(trigramcounts[t[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('START', 'START', 'natural') 0\n",
      "('START', 'natural', 'language') 1\n",
      "('natural', 'language', 'processing') 2\n",
      "('language', 'processing', 'STOP') 3\n"
     ]
    }
   ],
   "source": [
    "from collections import deque as dq\n",
    "l=[\"natural\",\"language\",\"processing\"]\n",
    "c = dq(l)\n",
    "c\n",
    "c.extendleft(['START']*2)\n",
    "c.extend(['STOP'])\n",
    "c = list(c)\n",
    "for i in range(0,len(c)-2):\n",
    "    print(tuple(c[i:i+3]), i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter([('START','START','the')])\n",
    "cnt[('START','START','the')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt['hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This file assumes Python 3\n",
    "## To work with Python 2, you would need to adjust\n",
    "## at least: the print statements (remove parentheses)\n",
    "## and the instances of division (convert\n",
    "## arguments of / to floats), and possibly other things\n",
    "## -- I have not tested this.\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "test_sentence_tokens = ['a','fact','about','the','unicorn','is','the','same','as','an','alternative','fact','about','the','unicorn','.']\n",
    "\n",
    "words = brown.words()\n",
    "fdist1 = nltk.FreqDist(w.lower() for w in words)\n",
    "\n",
    "total_words = len(words)\n",
    "\n",
    "print('Frequency of tokens in sample sententence in Brown according to NLTK:')\n",
    "\n",
    "for word in test_sentence_tokens:\n",
    "    print(word,fdist1[word])\n",
    "\n",
    "input('Pausing: Hit Return when Ready.')\n",
    "\n",
    "print('Given that there are',total_words,'in the Brown Corpus, the unigram probability of these words')\n",
    "print('is as follows (rounded to 3 significant digits):')\n",
    "\n",
    "for word in test_sentence_tokens:\n",
    "    unigram_probability = fdist1[word]/total_words\n",
    "    print(word,float('%.3g' % unigram_probability))\n",
    "    ## print(word,round((fdist1[word]/total_words),3))\n",
    "\n",
    "input('Pausing: Hit Return when Ready.')\n",
    "\n",
    "## ADD convert single count items to OOV\n",
    "## make simple assumption about sentence endings,\n",
    "## and the position of START and END (sentence boundaries)\n",
    "\n",
    "words2 = []\n",
    "previous = 'EMPTY'\n",
    "sentences = 0\n",
    "for word in words:\n",
    "    if previous in ['EMPTY','.','?','!']:\n",
    "        ## insert word_boundaries at beginning of Brown,\n",
    "        ## and after end-of-sentence markers (overgenerate due to abbreviations, etc.)\n",
    "        words2.append('*start_end*')\n",
    "    if fdist1[word]==1:\n",
    "        ## words occurring only once are treated as Out of Vocabulary Words\n",
    "        words2.append('*oov*')\n",
    "    else:\n",
    "        words2.append(word)\n",
    "    previous = word\n",
    "words2.append('*start_end*') ## assume one additional *start_end* at the end of Brown\n",
    "\n",
    "fdist2 = nltk.FreqDist(w.lower() for w in words2)\n",
    "## get Unigram counts for all words occuring more than once\n",
    "## and also a count for OOV words\n",
    "\n",
    "print('There are',fdist2['*oov*'],'instances of OOVs')\n",
    "\n",
    "print('Unigram probabilities including OOV probabilities.')\n",
    "\n",
    "def get_unigram_probability(word):\n",
    "    if word in fdist1:\n",
    "        unigram_probability = fdist2[word]/total_words\n",
    "    else:\n",
    "        unigram_probability = fdist2['*oov*']/total_words\n",
    "    return(unigram_probability)\n",
    "\n",
    "for word in test_sentence_tokens:\n",
    "    unigram_probability = get_unigram_probability(word)\n",
    "    print(word,float('%.3g' % unigram_probability))\n",
    "\n",
    "input('Pausing: Hit Return when Ready.')\n",
    "## make new version that models Out of Vocabulary (OOV) words\n",
    "\n",
    "print('Calculating bigram counts for sentence, including bigrams with sentence boundaries, i.e., *BEGIN* and *END*')\n",
    "print('Assuming some idealizations: all periods, questions and exclamation marks end sentences;')\n",
    "\n",
    "bigrams = nltk.bigrams(w.lower() for w in words2)\n",
    "## get bigrams for words2 (words plus OOV)\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(bigrams)\n",
    "\n",
    "# for token1 in cfd:\n",
    "#     if not '*oov*' in cfd[token1]:\n",
    "#         cfd[token1]['*oov*']=1\n",
    "#         ## fudge so there can be no \n",
    "#         ## 0 bigram\n",
    "\n",
    "def multiply_list(inlist):\n",
    "    out = 1\n",
    "    for number in inlist:\n",
    "        out *= number\n",
    "    return(out)\n",
    "\n",
    "def get_bigram_probability(first,second):\n",
    "    if not second in cfd[first]:\n",
    "        print('Backing Off to Unigram Probability for',second)\n",
    "        unigram_probability = get_unigram_probability(second)\n",
    "        return(unigram_probability)\n",
    "    else:\n",
    "        bigram_frequency = cfd[first][second]\n",
    "    unigram_frequency = fdist2[first]\n",
    "    bigram_probability = bigram_frequency/unigram_frequency\n",
    "    return(bigram_probability)\n",
    "\n",
    "def calculate_bigram_freq_of_sentence_token_list(tokens):\n",
    "    prob_list = []\n",
    "    ## assume that 'START' precedes the first token\n",
    "    previous = '*start_end*'\n",
    "    for token in tokens:\n",
    "        if not token  in fdist2:\n",
    "            token = '*oov*'\n",
    "        next_probability = get_bigram_probability(previous,token)\n",
    "        print(previous,token,(float('%.3g' % next_probability)))\n",
    "        prob_list.append(next_probability)\n",
    "        previous = token\n",
    "    ## assume that 'END' follows the last token\n",
    "    next_probability = get_bigram_probability(previous,'*start_end*')\n",
    "    print(previous,'*start_end*',next_probability)\n",
    "    prob_list.append(next_probability)\n",
    "    probability = multiply_list(prob_list)\n",
    "    print('Total Probability',float('%.3g' % probability))\n",
    "    return(probability)\n",
    "\n",
    "\n",
    "\n",
    "result = calculate_bigram_freq_of_sentence_token_list(test_sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(Counter([1,2,3,4,5,1,2,1,6]).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(0.394551,1)\n",
    "2/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "if('h' in 'hello'):\n",
    "    print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=[1,2,3,4]\n",
    "l[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=(1,2,3)\n",
    "s[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.log2(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0625"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter(['a','b','c','a'])\n",
    "cnt2 = Counter(['a'])\n",
    "cnt3 = Counter()\n",
    "cnt3+=cnt\n",
    "cnt3+=cnt2\n",
    "cnt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
